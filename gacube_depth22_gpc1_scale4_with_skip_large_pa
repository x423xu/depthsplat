nohup: ignoring input
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/MinkowskiEngine-0.5.4-py3.10-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.
  warnings.warn(
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:24,088][dinov2][INFO] - using MLP layer as FFN
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/10
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
/data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:33,963][dinov2][INFO] - using MLP layer as FFN
10 GPUs available
[2025-09-03 14:14:33,972][dinov2][INFO] - using MLP layer as FFN
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
10 GPUs available
[2025-09-03 14:14:34,087][dinov2][INFO] - using MLP layer as FFN
[36mSaving outputs to /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa.[39m
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:34,195][dinov2][INFO] - using MLP layer as FFN
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:34,226][dinov2][INFO] - using MLP layer as FFN
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:34,273][dinov2][INFO] - using MLP layer as FFN
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:34,301][dinov2][INFO] - using MLP layer as FFN
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
Using cache found in /data0/xxy/torch_cache/hub/facebookresearch_dinov2_main
10 GPUs available
[2025-09-03 14:14:34,391][dinov2][INFO] - using MLP layer as FFN
10 GPUs available
[2025-09-03 14:14:34,410][dinov2][INFO] - using MLP layer as FFN
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/10
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/10
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/10
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/10
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/10
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/10
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/10
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/10
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/10
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 10 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: xxy. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.21.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /data2/xxy/code/voxelsplat/checkpoints/gscube-depth22-gpc1-scale4-with-skip-large-pa/wandb/run-20250903_141441-kmwqgqqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gscube-depth22-gpc1-scale4-with-skip-large-pa
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xxy/depthsplat
wandb: üöÄ View run at https://wandb.ai/xxy/depthsplat/runs/kmwqgqqu
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 8 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]
LOCAL_RANK: 9 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8,9]

  | Name    | Type                 | Params | Mode 
---------------------------------------------------------
0 | encoder | EncoderDepthSplat    | 65.0 M | train
1 | decoder | DecoderSplattingCUDA | 0      | train
2 | losses  | ModuleList           | 0      | train
---------------------------------------------------------
26.7 M    Trainable params
38.3 M    Non-trainable params
65.0 M    Total params
260.048   Total estimated model params size (MB)
1245      Modules in train mode
59        Modules in eval mode
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 33017
val: 1
test: 7286
[36mLoaded pretrained monodepth: pretrained/depth_anything_v2_vits.pth[39m
[36mLoaded pretrained mvdepth: pretrained/gmflow-scale1-things-e9887eda.pth[39m
[36mLoaded pretrained weights: /data0/xxy/code/depthsplat/checkpoints/re10k-256x256-depthsplat-small/checkpoints/epoch_27-step_300000.ckpt[39m
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]validation step 0; scene = ['306e2b7785657539']; context = [[14, 44]]
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /data0/xxy/conda_envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
Saving Gaussian cube at step 0...
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:15<00:00,  0.06it/s]EncoderDepthSplat(
  (depth_predictor): MultiViewUniMatch(
    (backbone): CNNEncoder(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu1): ReLU(inplace=True)
      (layer1): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
        (1): ResidualBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (layer2): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm3): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (downsample): Sequential(
            (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
            (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): ResidualBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (layer3): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (downsample): Sequential(
            (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): ResidualBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer): MultiViewFeatureTransformer(
      (layers): ModuleList(
        (0-5): 6 x TransformerBlock(
          (self_attn): TransformerLayer(
            (q_proj): Linear(in_features=128, out_features=128, bias=False)
            (k_proj): Linear(in_features=128, out_features=128, bias=False)
            (v_proj): Linear(in_features=128, out_features=128, bias=False)
            (merge): Linear(in_features=128, out_features=128, bias=False)
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (cross_attn_ffn): TransformerLayer(
            (q_proj): Linear(in_features=128, out_features=128, bias=False)
            (k_proj): Linear(in_features=128, out_features=128, bias=False)
            (v_proj): Linear(in_features=128, out_features=128, bias=False)
            (merge): Linear(in_features=128, out_features=128, bias=False)
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=1024, out_features=128, bias=False)
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (pretrained): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x NestedTensorBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (regressor): ModuleList(
      (0): Sequential(
        (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GroupNorm(8, 128, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
        (3): UNetModel(
          (input_blocks): ModuleList(
            (0): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
            )
            (2): Sequential(
              (0): Downsample(
                (op): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
              )
            )
            (3): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
            )
            (4): Sequential(
              (0): Downsample(
                (op): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
              )
            )
            (5): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (middle_block): Sequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (out_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): Identity()
            (2): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (out_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
          (output_blocks): ModuleList(
            (0): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
            )
            (1): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (2): Upsample(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (2): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
            (3): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): Upsample(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (4-5): 2 x Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (out): Sequential(
            (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (regressor_residual): ModuleList(
      (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
    )
    (depth_head): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (1): GELU(approximate='none')
        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
      )
    )
    (upsampler): DPTHead(
      (concat_projects): ModuleList(
        (0): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(353, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (projects): ModuleList(
        (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (resize_layers): ModuleList(
        (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))
        (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
        (2): Identity()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (scratch): Module(
        (layer1_rn): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (refinenet1): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (output_conv): Sequential(
          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
          (1): GELU(approximate='none')
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
          (3): GELU(approximate='none')
          (4): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (feature_upsampler): DPTHead(
    (concat_projects): ModuleList(
      (0): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(352, 96, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    )
    (projects): ModuleList(
      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (resize_layers): ModuleList(
      (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))
      (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
      (2): Identity()
      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (scratch): Module(
      (layer1_rn): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
  )
  (gaussian_adapter): GaussianAdapter()
  (gaussian_regressor): Sequential(
    (0): Conv2d(69, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GELU(approximate='none')
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (gaussian_head): Sequential(
    (0): Conv2d(132, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
    (1): GELU(approximate='none')
    (2): Conv2d(37, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
  )
  (gs_cube_encoder): GSCubeEncoder(
    (stem_layer): MinkConvBNRelu(
      (conv_layers): Sequential(
        (0): MinkowskiConvolution(in=132, out=48, kernel_size=[3, 3, 3], stride=[1, 1, 1], dilation=[1, 1, 1])
        (1): MinkowskiBatchNorm(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): MinkowskiReLU()
      )
    )
    (layers): ModuleList(
      (0): BasicLayer(
        window_size=5, depth=2, channel=48, num_heads=6, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=48, out_features=144, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=48, out_features=48, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=48, out_features=192, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=192, out_features=48, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=48, out_features=144, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=48, out_features=48, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=48, out_features=192, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=192, out_features=48, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (pool): MinkowskiMaxPooling(kernel_size=[5, 5, 5], stride=[5, 5, 5], dilation=[1, 1, 1])
        (downsample): GridKNNDownsample(
          kernel_size=16, stride=3, in_channels=48, out_channels=96
          (sp_pool): MinkowskiMaxPooling(kernel_size=[3, 3, 3], stride=[3, 3, 3], dilation=[1, 1, 1])
          (coords_pool): GridCoordsDown(
            (avg_pool): MinkowskiAvgPooling(kernel_size=[3, 3, 3], stride=[3, 3, 3], dilation=[1, 1, 1])
            (unpool): MinkowskiPoolingTranspose(kernel_size=[3, 3, 3], stride=[3, 3, 3], dilation=[1, 1, 1])
            (max_pool): MinkowskiMaxPooling(kernel_size=[3, 3, 3], stride=[3, 3, 3], dilation=[1, 1, 1])
          )
          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (linear): Linear(in_features=48, out_features=96, bias=False)
          (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (1): BasicLayer(
        window_size=7, depth=4, channel=96, num_heads=6, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
        (blocks): ModuleList(
          (0-3): 4 x SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        (downsample): GridKNNDownsample(
          kernel_size=16, stride=2, in_channels=96, out_channels=192
          (sp_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          (coords_pool): GridCoordsDown(
            (avg_pool): MinkowskiAvgPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (unpool): MinkowskiPoolingTranspose(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (max_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          )
          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (linear): Linear(in_features=96, out_features=192, bias=False)
          (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (2): BasicLayer(
        window_size=7, depth=9, channel=192, num_heads=12, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
        (blocks): ModuleList(
          (0-8): 9 x SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        (downsample): GridKNNDownsample(
          kernel_size=16, stride=2, in_channels=192, out_channels=384
          (sp_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          (coords_pool): GridCoordsDown(
            (avg_pool): MinkowskiAvgPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (unpool): MinkowskiPoolingTranspose(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (max_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          )
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (linear): Linear(in_features=192, out_features=384, bias=False)
          (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (3): BasicLayer(
        window_size=7, depth=4, channel=384, num_heads=24, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
        (blocks): ModuleList(
          (0-3): 4 x SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        (downsample): GridKNNDownsample(
          kernel_size=16, stride=2, in_channels=384, out_channels=384
          (sp_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          (coords_pool): GridCoordsDown(
            (avg_pool): MinkowskiAvgPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (unpool): MinkowskiPoolingTranspose(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
            (max_pool): MinkowskiMaxPooling(kernel_size=[2, 2, 2], stride=[2, 2, 2], dilation=[1, 1, 1])
          )
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (linear): Linear(in_features=384, out_features=384, bias=False)
          (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (4): BasicLayer(
        window_size=7, depth=4, channel=384, num_heads=24, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
        (blocks): ModuleList(
          (0-3): 4 x SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
      )
    )
    (upsamples): ModuleList(
      (0): Upsample(
        up_k=3, in_channels=384, out_channels=384, attn=True
        (linear1): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=384, out_features=384, bias=True)
        )
        (linear2): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=384, out_features=384, bias=True)
        )
        (block): BasicLayer(
          window_size=7, depth=1, channel=384, num_heads=24, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=True)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        )
      )
      (1): Upsample(
        up_k=3, in_channels=384, out_channels=192, attn=True
        (linear1): Sequential(
          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=192, out_features=192, bias=True)
        )
        (linear2): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=384, out_features=192, bias=True)
        )
        (block): BasicLayer(
          window_size=7, depth=1, channel=192, num_heads=12, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=True)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        )
      )
      (2): Upsample(
        up_k=3, in_channels=192, out_channels=96, attn=True
        (linear1): Sequential(
          (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=96, out_features=96, bias=True)
        )
        (linear2): Sequential(
          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=192, out_features=96, bias=True)
        )
        (block): BasicLayer(
          window_size=7, depth=1, channel=96, num_heads=6, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=True)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (pool): MinkowskiMaxPooling(kernel_size=[7, 7, 7], stride=[7, 7, 7], dilation=[1, 1, 1])
        )
      )
      (3): Upsample(
        up_k=3, in_channels=96, out_channels=48, attn=True
        (linear1): Sequential(
          (0): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=48, out_features=48, bias=True)
        )
        (linear2): Sequential(
          (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=96, out_features=48, bias=True)
        )
        (block): BasicLayer(
          window_size=5, depth=1, channel=48, num_heads=6, quant_size=4, cRSE=XYZ_RGB, fp16_mode=0
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=48, out_features=144, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=True)
                (proj): Linear(in_features=48, out_features=48, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=48, out_features=192, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=192, out_features=48, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (pool): MinkowskiMaxPooling(kernel_size=[5, 5, 5], stride=[5, 5, 5], dilation=[1, 1, 1])
        )
      )
    )
    (classifier): Sequential(
      (0): Linear(in_features=48, out_features=48, bias=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=48, out_features=13, bias=True)
    )
    (gs_cube_head): GSCubeHead(
      (l1): Sequential(
        (0): Linear(in_features=48, out_features=38, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=38, out_features=38, bias=True)
      )
    )
  )
  (position_head): Sequential(
    (0): Linear(in_features=48, out_features=3, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=3, out_features=3, bias=True)
  )
  (param_head): Sequential(
    (0): Linear(in_features=51, out_features=35, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=35, out_features=35, bias=True)
  )
  (dense_gaussian_adapter): DenseGaussianAdapter()
)
                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/33017 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/33017 [00:00<?, ?it/s] Out of memory error in encoder: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 30.00 MiB is free. Process 2759334 has 25.18 GiB memory in use. Process 2753987 has 416.00 MiB memory in use. Process 3313911 has 790.00 MiB memory in use. Process 2902585 has 1.82 GiB memory in use. Including non-PyTorch memory, this process has 19.31 GiB memory in use. Of the allocated memory 18.31 GiB is allocated by PyTorch, and 537.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 0, scene_names: ['0f93fdb52c6933cf', 'a3a5e373d876db0e']
Epoch 0:   0%|          | 1/33017 [00:03<29:08:14,  0.31it/s]Epoch 0:   0%|          | 1/33017 [00:03<29:08:50,  0.31it/s, v_num=gqqu, train/nog_pb=98158.0, train/nog_min=1537.0, train/psnr=20.60, loss/mse=0.00909, oom_batch=1.000]Out of memory error in encoder: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 52.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 273.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 0, scene_names: ['8c4e0111b36af46d', 'a5572e0144dfdcc2']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 82.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.70 GiB is allocated by PyTorch, and 317.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 1, scene_names: ['346afdcdd046633a', '9b0cc25536e4bee4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 40.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 318.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 2, scene_names: ['dd70439f8c63e2c9', 'b9423910edf24e68']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 162.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 158.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.37 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 482.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 3, scene_names: ['a7e5c2b2c8fe38eb', 'b7b940096bce069a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 28.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.49 GiB memory in use. Of the allocated memory 2.68 GiB is allocated by PyTorch, and 386.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 4, scene_names: ['0f45d62d9cc48346', '077a53a763c9b806']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 8.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.51 GiB memory in use. Of the allocated memory 2.75 GiB is allocated by PyTorch, and 331.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 5, scene_names: ['ecd718353f8f83a9', '5ca4aa0b2f78cfe9']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 560.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 160.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.36 GiB memory in use. Of the allocated memory 2.21 GiB is allocated by PyTorch, and 732.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 6, scene_names: ['695e66b2294400dd', 'b100ffbc2f9d79a7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 114.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.41 GiB memory in use. Of the allocated memory 2.65 GiB is allocated by PyTorch, and 330.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 7, scene_names: ['148fc99aeebdfd97', '1b99c4ec83830714']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 152.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.37 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 216.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 8, scene_names: ['a9cd1ed7693da0f3', 'c76d5c37993cd546']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 62.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.46 GiB memory in use. Of the allocated memory 2.78 GiB is allocated by PyTorch, and 243.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 9, scene_names: ['d75a2d501d06abb2', 'cafb7b387787db6b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 52.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.47 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 301.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 10, scene_names: ['fc454ba4fe081bcf', 'e4e10e6b388f3a91']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 34.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 336.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 11, scene_names: ['52d8989a74a35e76', '84f1b2387fb4ed36']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 36.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.89 GiB memory in use. Including non-PyTorch memory, this process has 4.13 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 269.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 12, scene_names: ['7bc331c6216a1cef', '022df199d99d736d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 52.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.31 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 271.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 13, scene_names: ['71b063c7314ecdc9', 'e8b215c693c8c6b4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 24.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.33 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 346.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 14, scene_names: ['1e81c23a212ee848', '3a63f4b9d1be99b6']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 110.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.25 GiB memory in use. Of the allocated memory 3.49 GiB is allocated by PyTorch, and 327.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 15, scene_names: ['17805e587507e9e2', '573fcfbc1fc792b8']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 164.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.20 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 234.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 16, scene_names: ['afde92ddb60d24e6', 'fc71136d1f759ede']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 422.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 144.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.22 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 202.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 17, scene_names: ['d531e2b870b59c49', '6665f0ac0dd3e352']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 122.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.51 GiB is allocated by PyTorch, and 301.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 18, scene_names: ['012785080357adfd', 'd1b677821157bbd7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 124.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.43 GiB is allocated by PyTorch, and 384.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 19, scene_names: ['894989e32e698e7d', '0f2c72ccf47c2cca']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 178.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 66.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.29 GiB memory in use. Of the allocated memory 3.51 GiB is allocated by PyTorch, and 351.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 20, scene_names: ['e2ad007f157fce09', '0524d3d2e823ed7e']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 120.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.24 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 254.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 21, scene_names: ['8337ea841729afad', '381676b229abbab1']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 306.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.06 GiB memory in use. Of the allocated memory 3.40 GiB is allocated by PyTorch, and 228.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 22, scene_names: ['1faad50855fe9160', '44ab1ad50cc2d30a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 220.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 216.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.15 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 281.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 23, scene_names: ['d821b84ef7e2c186', '7499489011622024']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 12.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.35 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 310.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 24, scene_names: ['773592c80a3cbf02', '0e0784064e9f9f21']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 26.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.33 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 315.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 25, scene_names: ['b3a29ef93f2d9418', '1e496656b9d98481']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 152.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.70 GiB memory in use. Including non-PyTorch memory, this process has 4.21 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 225.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 26, scene_names: ['afbec7cd6de2b2a8', '7e423d9bae58a197']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 14.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.20 GiB is allocated by PyTorch, and 251.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 0, scene_names: ['2bfe6a3923cf9a83', 'afe244c3a728abb6']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 100.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.79 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 363.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 1, scene_names: ['f564c7789e774725', 'd1a3f6c04d4d3bed']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 200.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.70 GiB memory in use. Of the allocated memory 7.96 GiB is allocated by PyTorch, and 304.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 2, scene_names: ['163b84997faf53a9', 'd665cdc87a69be72']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 108.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.79 GiB memory in use. Of the allocated memory 8.03 GiB is allocated by PyTorch, and 333.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 3, scene_names: ['0b28474d69aa302f', '466a81cd2b82849a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 60.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.83 GiB memory in use. Of the allocated memory 8.08 GiB is allocated by PyTorch, and 321.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 4, scene_names: ['94c654bd3e031bcb', '2946d1671bef8129']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 10.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.13 GiB is allocated by PyTorch, and 328.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 5, scene_names: ['5d3e380ba663ef98', '19941455202c8fdb']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 592.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 108.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.79 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 249.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 6, scene_names: ['75fe4c914a4035bc', 'cb721728aa3c15de']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 10.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.09 GiB is allocated by PyTorch, and 363.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 7, scene_names: ['d8df805e4b97b406', '9c4c0f649f46db99']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 558.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 190.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 8.02 GiB is allocated by PyTorch, and 254.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 8, scene_names: ['9c9608ac2ebd5912', '4411a6eb5a900e40']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 20.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.09 GiB is allocated by PyTorch, and 357.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 9, scene_names: ['b860b021bed5a650', '5ab26790899c76d3']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 406.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 36.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.18 GiB is allocated by PyTorch, and 244.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 10, scene_names: ['41d0f7e77a32f5ef', '5993e9de5182c2a8']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 92.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.36 GiB memory in use. Of the allocated memory 8.58 GiB is allocated by PyTorch, and 358.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 0, scene_names: ['e567c08401dda8a7', '985e2ea54fcdc17f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 2.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.67 GiB is allocated by PyTorch, and 353.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 1, scene_names: ['b2c77ad4b2ae4a5e', 'e4c54fe00077ef30']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 64.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.39 GiB memory in use. Of the allocated memory 8.59 GiB is allocated by PyTorch, and 367.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 2, scene_names: ['c29fa2d4c02c7ab0', '1a8246c03de406e8']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 832.00 KiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.65 GiB is allocated by PyTorch, and 375.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 3, scene_names: ['166d39209d7cc4c3', '4648b54a491a37e2']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 560.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 184.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.27 GiB memory in use. Of the allocated memory 8.54 GiB is allocated by PyTorch, and 304.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 4, scene_names: ['03aa892f294f56ed', 'f82e61bd6f3b60ef']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 18.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.43 GiB memory in use. Of the allocated memory 8.69 GiB is allocated by PyTorch, and 313.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 5, scene_names: ['7c7404497e896e02', '84e25ecb06005b3d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 16.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.44 GiB memory in use. Of the allocated memory 8.66 GiB is allocated by PyTorch, and 344.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 6, scene_names: ['5b0cc99d38736efe', '5d0ce5e557060f2c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 24.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.43 GiB memory in use. Of the allocated memory 8.68 GiB is allocated by PyTorch, and 316.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 7, scene_names: ['6aa01d2af0fe23b4', '4918a5aaaefaf869']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 100.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.35 GiB memory in use. Of the allocated memory 8.55 GiB is allocated by PyTorch, and 380.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 8, scene_names: ['bf01907dda08e73c', '3e9e39a206935fe5']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 56.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Of the allocated memory 8.61 GiB is allocated by PyTorch, and 362.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 9, scene_names: ['adb7e9569b2d54e8', 'd7ca8401b3a2387b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 44.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.41 GiB memory in use. Of the allocated memory 8.61 GiB is allocated by PyTorch, and 372.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 10, scene_names: ['86cfe990f194bb9f', 'dd249f6269133fd4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 412.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 66.00 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Process 3198325 has 1.67 GiB memory in use. Including non-PyTorch memory, this process has 4.32 GiB memory in use. Of the allocated memory 3.69 GiB is allocated by PyTorch, and 200.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 27, scene_names: ['cef2ba3c47cf9eda', '0802c69f642f0a48']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 50.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 304.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 28, scene_names: ['d5a9be470293e204', '14f431bca6f79d3b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 738.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 466.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.61 GiB memory in use. Of the allocated memory 4.97 GiB is allocated by PyTorch, and 201.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 29, scene_names: ['43c9cb3a1f008f2b', '0be127358da454de']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 30.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.24 GiB is allocated by PyTorch, and 364.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 30, scene_names: ['8f17b5780b30ab5a', '7715e2b33306ae7c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 106.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.96 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 312.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 31, scene_names: ['fb0239ef9a6badcd', '80cef9332c5c91c7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 762.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 418.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.65 GiB memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 222.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 32, scene_names: ['1300df0c4d61179f', '52dad29ccea8dfa7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 102.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 12.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.35 GiB is allocated by PyTorch, and 273.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 33, scene_names: ['d9a133a4747493d2', '36b937cc3684eddb']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 704.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 556.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 4.83 GiB is allocated by PyTorch, and 254.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 34, scene_names: ['10a2d0db5c8c0962', '1bd0890e0abf8718']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 310.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 76.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.99 GiB memory in use. Of the allocated memory 5.32 GiB is allocated by PyTorch, and 233.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 35, scene_names: ['e33b2a9076f25c4d', 'b6f07b59b3a5e1a0']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 56.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.27 GiB is allocated by PyTorch, and 308.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 36, scene_names: ['a5d5924c33cb5eec', '2026ccc2dc2be8fa']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 746.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 382.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.69 GiB memory in use. Of the allocated memory 5.06 GiB is allocated by PyTorch, and 201.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 37, scene_names: ['83db4c5e4607f168', '7754cd2454032d71']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 604.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 286.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.78 GiB memory in use. Of the allocated memory 5.17 GiB is allocated by PyTorch, and 181.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 38, scene_names: ['aab1107e09a45e35', 'ec4619c9a04ad474']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 756.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 438.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.63 GiB memory in use. Of the allocated memory 4.98 GiB is allocated by PyTorch, and 218.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 39, scene_names: ['da3a6901f12a6e59', '477f44d4c838a6eb']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 662.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 208.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.86 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 235.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 40, scene_names: ['f9eb323d5f901122', '36ecd2c090551e5b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 2.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.06 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 444.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 41, scene_names: ['8ed6bd5510e508fa', 'ae8bc0141db29362']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 8.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 321.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 42, scene_names: ['778b069f291493b3', 'f421d632fde08451']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 54.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 393.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 43, scene_names: ['2c8487ab01c71a50', 'a9b7950720515148']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 646.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 196.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.87 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 252.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 44, scene_names: ['4b6dac5ca4c29f5e', '8a18643ea91b331c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 596.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 272.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.79 GiB memory in use. Of the allocated memory 5.14 GiB is allocated by PyTorch, and 227.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 45, scene_names: ['66599a3f0ff6eebb', '7ba6259f378c70f8']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 28.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.25 GiB is allocated by PyTorch, and 353.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 46, scene_names: ['144796f0d553a90b', '1e6a2c93047453fe']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 36.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.42 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 360.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 11, scene_names: ['9b8fbe92c8e15c4b', 'e73c88b002d4d7ca']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 26.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.43 GiB memory in use. Of the allocated memory 8.69 GiB is allocated by PyTorch, and 307.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 12, scene_names: ['36b91d574778ca45', '5de7a8c05f79bd08']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 832.00 KiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.73 GiB is allocated by PyTorch, and 296.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 13, scene_names: ['835c947a070ff657', '35e7870c4f60c02e']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 72.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.38 GiB memory in use. Of the allocated memory 8.48 GiB is allocated by PyTorch, and 480.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 14, scene_names: ['0117ac93ca7f4348', 'bacacdda6de65407']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 490.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 18.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.43 GiB memory in use. Of the allocated memory 8.70 GiB is allocated by PyTorch, and 301.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 15, scene_names: ['f8991aee27bc5030', 'd6038e785afd0093']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 154.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 28.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.42 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 356.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 16, scene_names: ['729e1efcddd5058a', '111008339f6622fa']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 124.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.33 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 401.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 17, scene_names: ['22eb3d3c8b5d65e4', '70c3d5d3e8a83e7d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 196.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.26 GiB memory in use. Of the allocated memory 8.51 GiB is allocated by PyTorch, and 321.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 18, scene_names: ['f3d11118a0c4506d', '301b15f94d691805']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 90.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.36 GiB memory in use. Of the allocated memory 8.55 GiB is allocated by PyTorch, and 391.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 19, scene_names: ['3dfdc1b6c7b658e3', '5b3422e1ad0e3f86']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 54.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Of the allocated memory 8.62 GiB is allocated by PyTorch, and 353.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 20, scene_names: ['73d0cc9f88dd47a4', '25f120b63c777a3d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 4.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.72 GiB is allocated by PyTorch, and 301.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 21, scene_names: ['ab68591f26d9260e', '711ade236bebd618']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 326.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.57 GiB memory in use. Of the allocated memory 7.86 GiB is allocated by PyTorch, and 281.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 11, scene_names: ['1b64868112b2ec1b', 'e1488d5d2d21df06']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 24.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 290.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 12, scene_names: ['1513f6c74e68f885', 'd7a372d91ada0e4d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 44.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.85 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 315.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 13, scene_names: ['47a6a231c819c5a2', '0dab28a141f4539b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 26.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.09 GiB is allocated by PyTorch, and 346.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 14, scene_names: ['23174a6cd65a0731', 'a367a7587df5e00c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 36.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 422.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 15, scene_names: ['618e278e25b9d1a7', '204e745bd032ba48']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 56.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.84 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 295.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 16, scene_names: ['c7f49b38fd7eefb3', 'ef77f5e4ef9e62b6']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 30.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.19 GiB is allocated by PyTorch, and 242.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 17, scene_names: ['c60844368cea2e56', 'a6ff3ec9a7d32fe7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 12.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.17 GiB is allocated by PyTorch, and 282.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 18, scene_names: ['c6d6e4afbfc319bb', 'd6443c4840318df3']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 162.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 52.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.84 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 409.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 19, scene_names: ['c371d1ab17ce1fc6', 'fd89bf7100bbab61']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 620.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 474.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.43 GiB memory in use. Of the allocated memory 7.74 GiB is allocated by PyTorch, and 257.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 20, scene_names: ['de9f8a0f0fdd4255', 'aa218c104399ce72']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 72.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.82 GiB memory in use. Of the allocated memory 8.07 GiB is allocated by PyTorch, and 328.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 21, scene_names: ['9f73234c746c169a', '3237356236c9ee91']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 80.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.98 GiB memory in use. Of the allocated memory 5.05 GiB is allocated by PyTorch, and 505.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 47, scene_names: ['c7ebbbd1d9644037', 'fadb7c559ead97a4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 90.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.97 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 326.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 48, scene_names: ['042a55e78c9a78bf', '7645fe1e7ec6456d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 14.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 314.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 49, scene_names: ['6bc8eee03ce7e03a', '4c768903212b223d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 2.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.06 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 351.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 50, scene_names: ['adb159b1165df8ff', 'a4b502cb83a02b68']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 8.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 342.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 51, scene_names: ['f02ac87191a94f65', 'b34dd66d4d3ce335']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 546.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 498.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.57 GiB memory in use. Of the allocated memory 4.93 GiB is allocated by PyTorch, and 218.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 52, scene_names: ['3994710eb422e589', '805e0eeb667b8d69']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 24.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.24 GiB is allocated by PyTorch, and 365.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 53, scene_names: ['9efc4c71d82f7576', '62008345be9d5772']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 138.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.93 GiB memory in use. Of the allocated memory 5.23 GiB is allocated by PyTorch, and 264.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 54, scene_names: ['5f036597e4f90cdd', 'ba301332e2986b17']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 118.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.95 GiB memory in use. Of the allocated memory 5.27 GiB is allocated by PyTorch, and 242.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 55, scene_names: ['a07c96f0aa5b4345', '334eb53845f3615f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 142.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 104.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.96 GiB memory in use. Of the allocated memory 5.13 GiB is allocated by PyTorch, and 405.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 56, scene_names: ['7069ae72dad7c42e', '738889f8821da74e']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 624.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 146.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.92 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 191.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 57, scene_names: ['e500f7fa2f8c7287', '7ae91e3d8ff76fa6']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 76.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.99 GiB memory in use. Of the allocated memory 5.13 GiB is allocated by PyTorch, and 434.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 58, scene_names: ['d602f5daed73334f', 'bcfb47fdd49eac62']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 104.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 102.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.96 GiB memory in use. Of the allocated memory 5.25 GiB is allocated by PyTorch, and 284.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 59, scene_names: ['471dbf0c87875c03', 'abebefeb0c4522ad']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 60.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.00 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 270.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 60, scene_names: ['991950cbc1228573', 'fe5c3e12d699d77f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 14.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.37 GiB is allocated by PyTorch, and 251.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 61, scene_names: ['75ad4bc6ef1969a8', '01c9eb355dca4a73']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 590.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.48 GiB memory in use. Of the allocated memory 4.86 GiB is allocated by PyTorch, and 193.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 62, scene_names: ['c702b9672415a091', '01e2bd7042fb4bd9']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 234.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.83 GiB memory in use. Of the allocated memory 5.16 GiB is allocated by PyTorch, and 239.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 63, scene_names: ['233cebf44f00b4b0', 'f660faddf8e7c064']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 8.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.05 GiB memory in use. Of the allocated memory 5.32 GiB is allocated by PyTorch, and 299.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 64, scene_names: ['a4c5e65a8a1fba1a', 'ea357cfdb1ebe90d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 20.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.41 GiB is allocated by PyTorch, and 198.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 65, scene_names: ['cf579459b0fb66df', 'bb518b3a290d5e83']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 40.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.02 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 416.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 66, scene_names: ['bb717dac4971645c', 'e54e0ffb622d8e56']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 170.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 124.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.77 GiB memory in use. Of the allocated memory 8.02 GiB is allocated by PyTorch, and 326.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 22, scene_names: ['9af4b91c0bdc2106', 'f1356a07278c0403']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 426.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 62.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.83 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 339.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 23, scene_names: ['c0955bccede2208c', 'a62ed7edac42b08f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 832.00 KiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 410.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 24, scene_names: ['68d8c862333a1684', '4f9716bb3dc7feec']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 164.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 144.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.75 GiB memory in use. Of the allocated memory 7.98 GiB is allocated by PyTorch, and 339.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 25, scene_names: ['b67e67bf7ff9457d', 'e1cb363436e80224']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 360.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 212.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.69 GiB memory in use. Of the allocated memory 7.91 GiB is allocated by PyTorch, and 349.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 26, scene_names: ['0dfbccbfd1f98e92', 'eef16f49751028f7']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 4.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 345.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 27, scene_names: ['754c5f8506aad9ca', 'b008df0261874734']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 8.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 336.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 28, scene_names: ['adbfdd15bb9409f4', 'd60aaaf09b0efb55']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 832.00 KiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 314.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 29, scene_names: ['6b6f032d1c33702f', '905b1855f110a802']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 106.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.79 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 348.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 30, scene_names: ['32a8b21bf64fa43b', '0b8e5155f6823656']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 408.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 146.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.75 GiB memory in use. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 281.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 31, scene_names: ['2f2c4bd219a3c52e', 'feced30301b43ffd']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 28.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 284.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 32, scene_names: ['6e469635fdfee4d0', '1fc2dc296ec26e73']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 38.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.41 GiB memory in use. Of the allocated memory 8.65 GiB is allocated by PyTorch, and 335.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 22, scene_names: ['de8e8e917ea9ecff', '0e3c0134e244b22b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 56.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Of the allocated memory 8.66 GiB is allocated by PyTorch, and 312.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 23, scene_names: ['97efceb3be1bd8df', '640e53b0da14830d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 8.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.44 GiB memory in use. Of the allocated memory 8.75 GiB is allocated by PyTorch, and 268.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 24, scene_names: ['403d96fa5b051cd6', '9ad1ef56f0d00649']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 90.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.36 GiB memory in use. Of the allocated memory 8.46 GiB is allocated by PyTorch, and 483.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 25, scene_names: ['e0ea545b60dd789d', '5e29bcfae9e5fb61']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 40.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.41 GiB memory in use. Of the allocated memory 8.70 GiB is allocated by PyTorch, and 279.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 26, scene_names: ['4dcb4479e2490a95', '54bffb953d877965']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 6.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 271.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 27, scene_names: ['12239f08fc659d7c', '1255b5d61f20b9a8']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 12.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.44 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 379.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 28, scene_names: ['292aac2b17294096', '2b1892610e2c4d6e']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 72.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.38 GiB memory in use. Of the allocated memory 8.58 GiB is allocated by PyTorch, and 371.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 29, scene_names: ['f1c70b01433e528f', '86af7cd006b63f11']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 122.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.33 GiB memory in use. Of the allocated memory 8.60 GiB is allocated by PyTorch, and 302.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 30, scene_names: ['f6eb748a01065695', 'e6910569bc347ba2']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 2.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Of the allocated memory 8.67 GiB is allocated by PyTorch, and 347.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 31, scene_names: ['aab67a3597f28686', 'c357fbd8aca05570']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 30.81 MiB is free. Process 3171626 has 38.07 GiB memory in use. Including non-PyTorch memory, this process has 9.42 GiB memory in use. Of the allocated memory 8.73 GiB is allocated by PyTorch, and 262.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 32, scene_names: ['8c9d5935dacf2a41', 'ef191187096ac71a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 614.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 472.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.60 GiB memory in use. Of the allocated memory 4.95 GiB is allocated by PyTorch, and 215.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 67, scene_names: ['97b507f2dbe2fb62', '1aa0de2130041622']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 32.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 289.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 68, scene_names: ['b489a5e69f2103d9', '80c93f2de4900d46']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 202.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.86 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 341.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 69, scene_names: ['b999e8b53f9bda12', 'a24deaa8e7512616']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 18.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 333.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 70, scene_names: ['6a492168e5974dea', '7a96795256185ae0']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 772.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 536.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.54 GiB memory in use. Of the allocated memory 4.89 GiB is allocated by PyTorch, and 214.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 71, scene_names: ['17de084707349f55', '07cb8141561cae19']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 616.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 324.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.74 GiB memory in use. Of the allocated memory 5.08 GiB is allocated by PyTorch, and 238.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 72, scene_names: ['86b6bf5ba97aca90', 'ddb77cfff0de0fc4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 52.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 287.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 73, scene_names: ['78a1eb48fb32e7c3', '6c461ad05bd11de0']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 542.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 528.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.54 GiB memory in use. Of the allocated memory 4.87 GiB is allocated by PyTorch, and 245.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 74, scene_names: ['42263054a7130d82', 'e4dd9a986d826d13']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 18.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.36 GiB is allocated by PyTorch, and 254.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 75, scene_names: ['cac37e977aa56490', '7426a108ff0bd8f0']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 724.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 554.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 4.87 GiB is allocated by PyTorch, and 221.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 76, scene_names: ['5b2278c4da769f90', '19e7498b055cd18a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 832.00 KiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 357.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 33, scene_names: ['d24dd458d5427d7d', '3122465597e9f593']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 14.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.03 GiB is allocated by PyTorch, and 427.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 34, scene_names: ['d6045d186161558e', '7229136d82d37912']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 30.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.08 GiB is allocated by PyTorch, and 360.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 35, scene_names: ['846f5168be2ffd1d', 'ffd0c7e94f23c5f3']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 14.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.13 GiB is allocated by PyTorch, and 325.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 36, scene_names: ['b7614724a495cd4b', '4ede83a5aea0010f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 102.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 26.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 402.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 37, scene_names: ['c1289a667237cd36', 'f28b0fa065688011']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 2.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.16 GiB is allocated by PyTorch, and 303.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 38, scene_names: ['b9aea5da640d739a', '2437ee5f02184c6d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 16.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 337.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 39, scene_names: ['0ee1992781f90c07', '94c9aa266f266590']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 30.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 395.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 40, scene_names: ['737c9d0602c08ef1', '59fe83675dae05d3']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 38.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.05 GiB is allocated by PyTorch, and 382.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 41, scene_names: ['5900d44f690b1ac8', 'e870602940137494']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 6.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 304.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 42, scene_names: ['1e1fb4e17b7db635', '50e6793206186822']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 42.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.85 GiB memory in use. Of the allocated memory 8.08 GiB is allocated by PyTorch, and 348.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 43, scene_names: ['557bfbc51cc423df', '149658fe965b3e8e']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 104.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 54.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 282.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 77, scene_names: ['a979210d30d8dc29', '270bc5262172dcde']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 28.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 327.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 78, scene_names: ['000d2139ad05dc85', '7d499b35c809b2f3']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 552.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 482.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.59 GiB memory in use. Of the allocated memory 4.95 GiB is allocated by PyTorch, and 204.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 79, scene_names: ['ef11eb76ba318a0f', 'c0f439bc475d053c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 74.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.99 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 347.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 80, scene_names: ['8ace2dc308ef63df', '79b558e7de32d04b']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 28.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.35 GiB is allocated by PyTorch, and 258.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 81, scene_names: ['e9916ce512982a66', 'f548180c554e2c94']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 66.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.00 GiB memory in use. Of the allocated memory 5.11 GiB is allocated by PyTorch, and 461.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 82, scene_names: ['66fbd45f9b7a5685', '46f7ce5962d1e536']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 26.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 422.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 83, scene_names: ['742da951005a3ff5', 'f69ea3d4400b7e81']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 26.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.33 GiB is allocated by PyTorch, and 277.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 84, scene_names: ['32d7ff686f72221e', '0befde1c678b05de']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 190.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 64.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.00 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 305.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 85, scene_names: ['d670e439d6d9ae8f', 'ae76b3fefecf973d']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 44.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.02 GiB memory in use. Of the allocated memory 5.24 GiB is allocated by PyTorch, and 350.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 86, scene_names: ['1f696b93c71675fd', 'acd0ea80c7595840']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 612.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 346.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.72 GiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 260.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 87, scene_names: ['93ef2f0bd081e8eb', '27aaacb0d6f48092']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 142.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 58.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.00 GiB memory in use. Of the allocated memory 5.01 GiB is allocated by PyTorch, and 568.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 88, scene_names: ['c4739a6b632f2588', '0dd1d031a21641ab']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 56.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.01 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 265.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 89, scene_names: ['8db7fda9b183d8e0', '75d5f44174475006']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 24.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 434.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 90, scene_names: ['a6a3c4ea1adc0a50', '82cdd6ac972ce6ba']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 18.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 340.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 91, scene_names: ['e2c8167e5f23e382', '5e4ac2c13ad0dad4']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 34.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.03 GiB memory in use. Of the allocated memory 5.01 GiB is allocated by PyTorch, and 592.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 92, scene_names: ['b145a55f25369072', '7482427ddbfafddc']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 724.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 498.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.57 GiB memory in use. Of the allocated memory 4.94 GiB is allocated by PyTorch, and 198.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 93, scene_names: ['c1bbf4879c1546b1', '30acb971c9074776']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 24.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.04 GiB memory in use. Of the allocated memory 5.27 GiB is allocated by PyTorch, and 340.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 94, scene_names: ['bd2dcb0a4d2b8645', '4414c58676c9952a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 622.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 552.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 5.52 GiB memory in use. Of the allocated memory 4.88 GiB is allocated by PyTorch, and 209.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 95, scene_names: ['7dbd9631683c5ae8', 'a1331ad1467f9dbb']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 7 has a total capacity of 47.53 GiB of which 4.94 MiB is free. Process 3183345 has 7.18 GiB memory in use. Process 3187495 has 21.69 GiB memory in use. Process 3196043 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 6.06 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 322.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 96, scene_names: ['ea65e9d79b43d884', 'aec0147e1835cc70']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 62.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.83 GiB memory in use. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 369.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 44, scene_names: ['ba66ab644596fa12', 'f5bc5f1993328821']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 542.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 132.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.76 GiB memory in use. Of the allocated memory 8.05 GiB is allocated by PyTorch, and 283.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 45, scene_names: ['0bc354faf077bd74', 'ebdc2cc016eb85d0']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 30.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.86 GiB memory in use. Of the allocated memory 8.14 GiB is allocated by PyTorch, and 298.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 46, scene_names: ['5c49af63ed05b277', '08e0627969b8514a']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 114.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.78 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 341.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 47, scene_names: ['738c891192860c9e', 'e2bc31428756b38f']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 52.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.84 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 353.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 48, scene_names: ['46a0fbe5394eecb2', '87d369842c66fc64']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 70.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.82 GiB memory in use. Of the allocated memory 8.11 GiB is allocated by PyTorch, and 288.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 49, scene_names: ['ecd5e56d2d3038e7', '73578fbad99d7332']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 70.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.82 GiB memory in use. Of the allocated memory 8.08 GiB is allocated by PyTorch, and 316.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 50, scene_names: ['d72247691e33a958', '265dcef75191f694']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 2.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.89 GiB memory in use. Of the allocated memory 8.20 GiB is allocated by PyTorch, and 264.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 51, scene_names: ['cd70e539713918da', '53cf0ccf9f3cd287']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 146.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.75 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 485.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 52, scene_names: ['1996cd816b13217e', '1614afe6bba5154c']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 382.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 192.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 7.99 GiB is allocated by PyTorch, and 288.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 53, scene_names: ['e027873a0401fffc', '2d41459c943e1f71']
Out of memory error in encoder: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 47.53 GiB of which 8.81 MiB is free. Process 3171627 has 38.62 GiB memory in use. Including non-PyTorch memory, this process has 8.88 GiB memory in use. Of the allocated memory 8.14 GiB is allocated by PyTorch, and 317.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) at global_step: 0, batch_idx: 54, scene_names: ['308ec88c1ad9b768', 'b52a2802cd4234d6']
[rank: 9] Child process with PID 3198739 terminated with code -9. Forcefully terminating all other processes to avoid zombies üßü
Skipped bad example 70a864cc799c829d. Context shape was torch.Size([2, 3, 270, 480]) and target shape was torch.Size([4, 3, 270, 480]).
Skipped bad example 7ab1ca08282c3433. Context shape was torch.Size([2, 3, 360, 480]) and target shape was torch.Size([4, 3, 360, 480]).
